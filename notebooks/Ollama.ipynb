{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a3a75b",
   "metadata": {},
   "source": [
    "## **Convert all of these Anthropic examples to work with DeepSeek-R1 via Ollama.** \n",
    " \n",
    " ##**Anthropic Examples**## \n",
    " \n",
    " Sure, I can help you convert the Anthropic examples to work with DeepSeek-R1 via Ollama. Please provide the specific Anthropic examples you would like to convert. Here are some general guidelines and a template to get started:\n",
    "\n",
    " 1. Stream Thinking Example\n",
    " 2. Tools Example \n",
    " 3. Budget Comparison \n",
    " 4. Prompting Best Practices \n",
    " 5. Cost Analysis \n",
    "   \n",
    "###\n",
    "| **Anthropic Feature** | **DeepSeek-R1 Equivalent** |\n",
    "|----------------------|----------------------------|\n",
    "| `thinking={\"type\": \"enabled\"}` | Automatic `<think>` tags |\n",
    "| `budget_tokens=5000` | No budget needed - automatic |\n",
    "| `client.messages.stream()` | `client.chat(stream=True)` |\n",
    "| `block.type == \"thinking\"` | Parse `<think>...</think>` |\n",
    "| Token cost management | FREE local inference |\n",
    "| Tool calling | Manual calculation requests |\n",
    "| Response streaming events | Chunk-based content streaming |\n",
    "\n",
    "### Key Advantages of DeepSeek-R1:\n",
    "\n",
    "1. **üí∞ Cost**: Completely FREE vs Claude's $0.05-0.50 per analysis\n",
    "2. **üß† Thinking**: Always visible, no budget management needed  \n",
    "3. **üîí Privacy**: Runs locally, no cloud dependencies\n",
    "4. **‚ö° Performance**: No network latency, unlimited usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5844ea5",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## 2. Setting Up Your Environment using Astral uv\n",
    "\n",
    "Let's start by installing the necessary packages and setting up our API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685ee4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ollama\n",
    "from IPython.display import Markdown, display\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Use the working URL directly\n",
    "ollama_url = 'http://localhost:11434'  # Your confirmed working URL\n",
    "model_name = 'deepseek-r1:14b'\n",
    "\n",
    "# Create Ollama client\n",
    "client = ollama.Client(host=ollama_url)\n",
    "\n",
    "print(f\"‚úÖ Connected to Ollama at: {ollama_url}\")\n",
    "print(f\"ü§ñ Using model: {model_name}\")\n",
    "\n",
    "# Verify the model is available\n",
    "try:\n",
    "    models = client.list()\n",
    "    available_models = [m['name'] for m in models.get('models', [])]\n",
    "    print(f\"‚úÖ Found {len(available_models)} models\")\n",
    "    \n",
    "    if model_name in available_models:\n",
    "        print(f\"‚úÖ {model_name} is available and ready!\")\n",
    "    else:\n",
    "        print(f\"‚ùå {model_name} not found. Available models:\")\n",
    "        for model in available_models[:5]:  # Show first 5\n",
    "            print(f\"  - {model}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error verifying models: {e}\")\n",
    "\n",
    "# Test a simple chat\n",
    "def test_basic_chat():\n",
    "    print(f\"\\nüß™ Testing basic chat...\")\n",
    "    try:\n",
    "        response = client.chat(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hello! Just say 'Hi' back.\"}],\n",
    "            stream=False\n",
    "        )\n",
    "        \n",
    "        answer = response.get('message', {}).get('content', '')\n",
    "        print(f\"‚úÖ DeepSeek says: {answer}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Chat test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "if test_basic_chat():\n",
    "    print(f\"\\nüéâ Everything is working perfectly!\")\n",
    "    print(f\"You can now use the converted examples.\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Something went wrong with the chat test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16068e0d",
   "metadata": {},
   "source": [
    "### All of my Ollama.ai models are supported.\n",
    "\n",
    "| Model Name               | ID              | Size    | Last Modified |\n",
    "|--------------------------|-----------------|---------|---------------|\n",
    "| phi4-mini:3.8b           | 78fad5d182a7    | 2.5 GB  | 6 days ago    |\n",
    "| deepseek-r1:32b          | edba8017331d    | 19 GB   | 6 days ago    |\n",
    "| deepseek-r1:14b          | c333b7232bdb    | 9.0 GB  | 6 days ago    |\n",
    "| deepseek-r1:8b           | 6995872bfe4c    | 5.2 GB  | 6 days ago    |\n",
    "| deepseek-r1:1.5b         | e0979632db5a    | 1.1 GB  | 6 days ago    |\n",
    "| gemma3n:e4b              | 15cb39fd9394    | 7.5 GB  | 6 days ago    |\n",
    "| qwen2.5vl:7b             | 5ced39dfa4ba    | 6.0 GB  | 3 weeks ago   |\n",
    "| qwen2.5vl:32b            | 3edc3a52fe98    | 21 GB   | 3 weeks ago   |\n",
    "| mistral-small3.1:24b     | b9aaf0c2586a    | 15 GB   | 3 weeks ago   |\n",
    "| gemma3:27b               | a418f5838eaf    | 17 GB   | 4 weeks ago   |\n",
    "| gemma3:12b               | f4031aab637d    | 8.1 GB  | 4 weeks ago   |\n",
    "| gemma3:4b                | a2af6cc3eb7f    | 3.3 GB  | 4 weeks ago   |\n",
    "| phi4-reasoning:latest    | 47e2630ccbcd    | 11 GB   | 4 weeks ago   |\n",
    "| qwen2.5-coder:14b        | 9ec8897f747e    | 9.0 GB  | 4 weeks ago   |\n",
    "| qwen2.5-coder:7b         | dae161e27b0e    | 4.7 GB  | 4 weeks ago   |\n",
    "| qwen3:14b                | bdbd181c33f2    | 9.3 GB  | 4 weeks ago   |\n",
    "| qwen3:8b                 | 500a1f067a9f    | 5.2 GB  | 4 weeks ago   |\n",
    "| phi4:latest              | ac896e5b8b34    | 9.1 GB  | 6 months ago  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1d9995",
   "metadata": {},
   "source": [
    "<a id='basic-usage'></a>\n",
    "## 3. Basic Usage\n",
    "\n",
    "Let's start with a simple example to see extended thinking in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f873cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your basic thinking example (converted)\n",
    "def basic_thinking_example():\n",
    "    response = client.chat(\n",
    "        model='deepseek-r1:14b',\n",
    "        messages=[{\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"What is 27 * 453? Show me how you calculate this step by step.\"\n",
    "        }],\n",
    "        stream=False\n",
    "    )\n",
    "    \n",
    "    full_response = response.get('message', {}).get('content', '')\n",
    "    \n",
    "    # Parse thinking and answer\n",
    "    if '<think>' in full_response:\n",
    "        thinking_start = full_response.find('<think>') + 7\n",
    "        thinking_end = full_response.find('</think>')\n",
    "        thinking = full_response[thinking_start:thinking_end].strip()\n",
    "        answer = full_response[thinking_end + 8:].strip()\n",
    "        \n",
    "        print(\"ü§î DeepSeek's Thinking:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(thinking)\n",
    "        print(\"-\" * 50)\n",
    "        print(\"\\n‚úÖ Final Answer:\")\n",
    "        print(answer)\n",
    "    else:\n",
    "        print(\"‚úÖ Response:\")\n",
    "        print(full_response)\n",
    "\n",
    "# Run it\n",
    "basic_thinking_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e4d362",
   "metadata": {},
   "source": [
    "# **No, this is NOT valid for DeepSeek-R1.** Here's the key difference:\n",
    "\n",
    "## Anthropic Claude vs DeepSeek-R1 Thinking\n",
    "\n",
    "### ‚ùå **Anthropic Claude (Manual Configuration)**\n",
    "```python\n",
    "thinking={\n",
    "    \"type\": \"enabled\",        # Must explicitly enable\n",
    "    \"budget_tokens\": 5000     # Must set token budget\n",
    "}\n",
    "```\n",
    "\n",
    "### ‚úÖ **DeepSeek-R1 (Automatic)**\n",
    "```python\n",
    "# No thinking parameters needed!\n",
    "client.chat(\n",
    "    model='deepseek-r1:14b',\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Your question\"}]\n",
    ")\n",
    "# Thinking happens automatically in <think>...</think> tags\n",
    "```\n",
    "\n",
    "## Key Differences:\n",
    "\n",
    "| **Feature** | **Anthropic Claude** | **DeepSeek-R1** |\n",
    "|-------------|---------------------|-----------------|\n",
    "| **Thinking Control** | Manual (`thinking={\"type\": \"enabled\"}`) | ‚úÖ **Automatic** |\n",
    "| **Token Budget** | Required (`budget_tokens: 1024-32000`) | ‚úÖ **No budget needed** |\n",
    "| **Thinking Output** | Separate block/summary | ‚úÖ **Full process in `<think>` tags** |\n",
    "| **Cost** | Thinking tokens billed as output | ‚úÖ **FREE (local)** |\n",
    "| **Configuration** | Complex parameter management | ‚úÖ **Zero configuration** |\n",
    "\n",
    "## DeepSeek-R1 Thinking Behavior:\n",
    "\n",
    "```python\n",
    "# Simple question = shorter thinking\n",
    "response = client.chat(messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}])\n",
    "# Result: <think>Simple addition: 2+2=4</think>The answer is 4.\n",
    "\n",
    "# Complex question = extensive thinking  \n",
    "response = client.chat(messages=[{\"role\": \"user\", \"content\": \"Design a REST API...\"}])\n",
    "# Result: <think>[5000+ characters of detailed reasoning]</think>[Final answer]\n",
    "```\n",
    "\n",
    "## Why DeepSeek-R1 is Simpler:\n",
    "\n",
    "1. **üéØ No Parameter Management**: Just ask your question\n",
    "2. **üß† Intelligent Scaling**: Thinking depth automatically matches complexity\n",
    "3. **üí∞ No Token Costs**: Think as much as needed without cost concerns\n",
    "4. **üîç Full Transparency**: See the complete thought process, not just summaries\n",
    "\n",
    "**Bottom Line**: DeepSeek-R1's thinking is **automatic, free, and transparent** - no configuration needed like Claude's manual thinking parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c805e89",
   "metadata": {},
   "source": [
    "<a id='thinking-blocks'></a>\n",
    "## 4. Understanding Thinking Blocks\n",
    "\n",
    "Let's explore how thinking blocks work and what information they contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9b7d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_thinking_blocks():\n",
    "    \"\"\"Demonstrate the structure of DeepSeek-R1 thinking blocks\"\"\"\n",
    "    \n",
    "    response = client.chat(\n",
    "        model='deepseek-r1:14b',\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"I have a list of numbers: [15, 23, 8, 42, 16, 4, 30, 12].\n",
    "            \n",
    "            Please:\n",
    "            1. Find the median\n",
    "            2. Calculate the mean\n",
    "            3. Identify any outliers using the IQR method\n",
    "            4. Suggest what this data might represent\"\"\"\n",
    "        }],\n",
    "        stream=False\n",
    "    )\n",
    "    \n",
    "    # Get the full response\n",
    "    full_response = response.get('message', {}).get('content', '')\n",
    "    \n",
    "    # Analyze the response structure\n",
    "    print(\"üìä DeepSeek-R1 Response Structure Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Parse thinking vs final answer\n",
    "    if '<think>' in full_response and '</think>' in full_response:\n",
    "        thinking_start = full_response.find('<think>') + 7\n",
    "        thinking_end = full_response.find('</think>')\n",
    "        thinking_content = full_response[thinking_start:thinking_end].strip()\n",
    "        final_answer = full_response[thinking_end + 8:].strip()\n",
    "        \n",
    "        print(f\"\\nResponse Structure:\")\n",
    "        print(f\"  Total Length: {len(full_response)} characters\")\n",
    "        print(f\"  Has Thinking Block: Yes\")\n",
    "        print(f\"  Thinking Length: {len(thinking_content)} characters\")\n",
    "        print(f\"  Final Answer Length: {len(final_answer)} characters\")\n",
    "        print(f\"  Thinking Ratio: {len(thinking_content)/(len(full_response))*100:.1f}%\")\n",
    "        \n",
    "        print(f\"\\nü§î THINKING PROCESS:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(thinking_content)\n",
    "        \n",
    "        print(f\"\\n‚úÖ FINAL ANSWER:\")\n",
    "        print(\"-\" * 40)\n",
    "        display(Markdown(final_answer))\n",
    "        \n",
    "        # Additional analysis\n",
    "        print(f\"\\nüìà THINKING ANALYSIS:\")\n",
    "        print(\"-\" * 40)\n",
    "        thinking_lines = thinking_content.split('\\n')\n",
    "        print(f\"  Lines in thinking: {len(thinking_lines)}\")\n",
    "        print(f\"  Words in thinking: {len(thinking_content.split())}\")\n",
    "        \n",
    "        # Look for mathematical reasoning patterns\n",
    "        math_keywords = ['calculate', 'median', 'mean', 'average', 'sort', 'IQR', 'outlier']\n",
    "        found_keywords = [kw for kw in math_keywords if kw.lower() in thinking_content.lower()]\n",
    "        print(f\"  Mathematical concepts mentioned: {found_keywords}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nResponse Structure:\")\n",
    "        print(f\"  Total Length: {len(full_response)} characters\")\n",
    "        print(f\"  Has Thinking Block: No\")\n",
    "        print(f\"  Response Type: Direct answer\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ RESPONSE:\")\n",
    "        print(\"-\" * 40)\n",
    "        display(Markdown(full_response))\n",
    "    \n",
    "    return full_response\n",
    "\n",
    "# Run the analysis\n",
    "result = analyze_thinking_blocks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c95eb1",
   "metadata": {},
   "source": [
    "**No, this is NOT valid for DeepSeek-R1.** Those specific features are unique to Anthropic's Claude 4 implementation. Here's the comparison:\n",
    "\n",
    "## Anthropic Claude 4 vs DeepSeek-R1 Thinking\n",
    "\n",
    "| **Feature** | **Anthropic Claude 4** | **DeepSeek-R1** |\n",
    "|-------------|------------------------|------------------|\n",
    "| **Summarization** | ‚úÖ Provides summarized thinking | ‚ùå Shows FULL thinking process |\n",
    "| **Billing Model** | üí∞ Charged for full thinking tokens | üí∞ FREE (local inference) |\n",
    "| **Signature Verification** | ‚úÖ Cryptographic signatures | ‚ùå No signatures needed |\n",
    "| **Privacy Controls** | ‚úÖ Controlled thinking exposure | ‚úÖ Full local privacy |\n",
    "\n",
    "## DeepSeek-R1 Thinking Characteristics:\n",
    "\n",
    "### 1. **Full Thinking Display** (Not Summarized)\n",
    "```python\n",
    "# DeepSeek-R1 shows EVERYTHING in <think> tags\n",
    "response = \"\"\"<think>\n",
    "Let me work through this step by step...\n",
    "First I need to calculate 27 * 453...\n",
    "27 * 453 = 27 * (450 + 3) = 27 * 450 + 27 * 3\n",
    "27 * 450 = 27 * 45 * 10 = 1215 * 10 = 12,150\n",
    "27 * 3 = 81\n",
    "So 27 * 453 = 12,150 + 81 = 12,231\n",
    "</think>\n",
    "\n",
    "The answer is 27 √ó 453 = 12,231\"\"\"\n",
    "```\n",
    "\n",
    "### 2. **No Token Billing** (Free Local)\n",
    "- No \"thinking budget\" needed\n",
    "- No charge per token\n",
    "- Unlimited thinking depth\n",
    "\n",
    "### 3. **No Cryptographic Signatures**\n",
    "- No verification system\n",
    "- Direct model output\n",
    "- Trust based on model reliability\n",
    "\n",
    "### 4. **Complete Transparency**\n",
    "- You see the actual thinking process\n",
    "- No hidden reasoning steps\n",
    "- Full visibility into model reasoning\n",
    "\n",
    "## What This Means for You:## The Key Difference:\n",
    "\n",
    "**Anthropic Claude 4** gives you a \"polished summary\" of thinking with enterprise features.\n",
    "\n",
    "**DeepSeek-R1** gives you the \"raw, unfiltered thinking process\" with complete transparency.\n",
    "\n",
    "Think of it like:\n",
    "- **Claude 4**: A professional report with executive summary\n",
    "- **DeepSeek-R1**: The researcher's complete notebook with all work shown\n",
    "\n",
    "Both are valuable, but for **learning and understanding AI reasoning**, DeepSeek-R1 actually gives you MORE insight into how the model thinks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2b6885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepseek_thinking_reality():\n",
    "    \"\"\"Demonstrate actual DeepSeek-R1 thinking characteristics\"\"\"\n",
    "    \n",
    "    print(\"üîç DeepSeek-R1 Thinking Reality Check\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test to show actual thinking behavior\n",
    "    response = client.chat(\n",
    "        model='deepseek-r1:14b',\n",
    "        messages=[{\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Calculate 27 * 453 and show your work clearly.\"\n",
    "        }],\n",
    "        stream=False\n",
    "    )\n",
    "    \n",
    "    full_response = response.get('message', {}).get('content', '')\n",
    "    \n",
    "    print(\"üß† WHAT YOU ACTUALLY GET:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if '<think>' in full_response and '</think>' in full_response:\n",
    "        thinking_start = full_response.find('<think>') + 7\n",
    "        thinking_end = full_response.find('</think>')\n",
    "        thinking_content = full_response[thinking_start:thinking_end].strip()\n",
    "        final_answer = full_response[thinking_end + 8:].strip()\n",
    "        \n",
    "        print(\"‚úÖ Full thinking process visible:\")\n",
    "        print(f\"   Length: {len(thinking_content)} characters\")\n",
    "        print(f\"   Words: {len(thinking_content.split())} words\")\n",
    "        print(f\"   Complete reasoning: YES\")\n",
    "        print(f\"   Summarized: NO\")\n",
    "        print(f\"   Cryptographic signature: NO\")\n",
    "        print(f\"   Cost: $0.00 (FREE)\")\n",
    "        \n",
    "        print(f\"\\nüéØ THINKING SAMPLE (first 500 chars):\")\n",
    "        print(f\"   {thinking_content[:500]}...\")\n",
    "        \n",
    "        print(f\"\\nüí° FINAL ANSWER:\")\n",
    "        print(f\"   {final_answer}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No thinking block found in this response\")\n",
    "    \n",
    "    return full_response\n",
    "\n",
    "def anthropic_vs_deepseek_summary():\n",
    "    \"\"\"Summary of key differences\"\"\"\n",
    "    \n",
    "    print(f\"\\nüìä ANTHROPIC vs DEEPSEEK-R1 SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    differences = [\n",
    "        {\n",
    "            \"aspect\": \"Thinking Visibility\",\n",
    "            \"anthropic\": \"Summarized snippets\",\n",
    "            \"deepseek\": \"Complete full process\",\n",
    "            \"winner\": \"DeepSeek-R1\"\n",
    "        },\n",
    "        {\n",
    "            \"aspect\": \"Cost Model\", \n",
    "            \"anthropic\": \"Pay per thinking token\",\n",
    "            \"deepseek\": \"Free local inference\",\n",
    "            \"winner\": \"DeepSeek-R1\"\n",
    "        },\n",
    "        {\n",
    "            \"aspect\": \"Token Management\",\n",
    "            \"anthropic\": \"Budget required\",\n",
    "            \"deepseek\": \"No limits needed\",\n",
    "            \"winner\": \"DeepSeek-R1\"\n",
    "        },\n",
    "        {\n",
    "            \"aspect\": \"Privacy\",\n",
    "            \"anthropic\": \"Cloud-based processing\",\n",
    "            \"deepseek\": \"Local-only processing\", \n",
    "            \"winner\": \"DeepSeek-R1\"\n",
    "        },\n",
    "        {\n",
    "            \"aspect\": \"Setup Complexity\",\n",
    "            \"anthropic\": \"API key only\",\n",
    "            \"deepseek\": \"Local installation\",\n",
    "            \"winner\": \"Anthropic\"\n",
    "        },\n",
    "        {\n",
    "            \"aspect\": \"Verification\",\n",
    "            \"anthropic\": \"Cryptographic signatures\",\n",
    "            \"deepseek\": \"Direct model output\",\n",
    "            \"winner\": \"Anthropic\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(f\"{'Aspect':<20} | {'Anthropic':<20} | {'DeepSeek-R1':<20} | {'Better'}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for diff in differences:\n",
    "        winner_symbol = \"üèÜ\" if diff['winner'] == \"DeepSeek-R1\" else \"‚≠ê\"\n",
    "        print(f\"{diff['aspect']:<20} | {diff['anthropic']:<20} | {diff['deepseek']:<20} | {winner_symbol} {diff['winner']}\")\n",
    "    \n",
    "    print(f\"\\nüéØ BOTTOM LINE:\")\n",
    "    print(f\"   ‚Ä¢ DeepSeek-R1: Better for transparency, cost, privacy\")\n",
    "    print(f\"   ‚Ä¢ Anthropic: Better for enterprise verification, ease of setup\")\n",
    "    print(f\"   ‚Ä¢ Both: Excellent reasoning capabilities\")\n",
    "\n",
    "def practical_implications():\n",
    "    \"\"\"What this means for your usage\"\"\"\n",
    "    \n",
    "    print(f\"\\nüí° PRACTICAL IMPLICATIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    implications = [\n",
    "        \"‚úÖ You see MORE thinking detail with DeepSeek-R1\",\n",
    "        \"‚úÖ No token budgets to manage or optimize\", \n",
    "        \"‚úÖ No surprise costs from long thinking processes\",\n",
    "        \"‚ùå No verification signatures (trust the model)\",\n",
    "        \"‚ùå More complex initial setup required\",\n",
    "        \"‚úÖ Complete privacy and data control\"\n",
    "    ]\n",
    "    \n",
    "    for implication in implications:\n",
    "        print(f\"   {implication}\")\n",
    "    \n",
    "    print(f\"\\nüöÄ RECOMMENDATION:\")\n",
    "    print(f\"   Use DeepSeek-R1 for: Learning, experimentation, cost-sensitive projects\")\n",
    "    print(f\"   Use Claude 4 for: Enterprise verification, quick API setup\")\n",
    "\n",
    "# Run the reality check\n",
    "result = deepseek_thinking_reality()\n",
    "anthropic_vs_deepseek_summary()\n",
    "practical_implications()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c0a6c4",
   "metadata": {},
   "source": [
    "No, the specific **streaming API structure** from Anthropic is not directly valid for DeepSeek-R1 via Ollama. Here's the comparison:\n",
    "\n",
    "## ‚ùå Anthropic Streaming (Not Valid for DeepSeek)\n",
    "\n",
    "```python\n",
    "# This is Anthropic-specific and won't work with DeepSeek-R1\n",
    "with client.messages.stream(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    thinking={\"type\": \"enabled\", \"budget_tokens\": 5000},\n",
    "    messages=[{\"role\": \"user\", \"content\": \"...\"}]\n",
    ") as stream:\n",
    "    for event in stream:\n",
    "        if event.type == \"content_block_start\":\n",
    "            # Anthropic-specific event structure\n",
    "        elif event.type == \"content_block_delta\":\n",
    "            # Anthropic-specific delta events\n",
    "```\n",
    "\n",
    "## ‚úÖ DeepSeek-R1 Streaming (What Actually Works)## Summary: What's Valid vs Invalid\n",
    "\n",
    "### ‚ùå **NOT Valid for DeepSeek-R1:**\n",
    "- `client.messages.stream()` - Different API\n",
    "- `thinking={\"type\": \"enabled\", \"budget_tokens\": 5000}` - No budget system\n",
    "- `event.type == \"content_block_start\"` - Different event structure\n",
    "- `event.delta.type == \"thinking_delta\"` - No delta events\n",
    "- Context managers (`with stream as s:`) - Different pattern\n",
    "\n",
    "### ‚úÖ **Valid Concepts (but different implementation):**\n",
    "- **Streaming responses** - Yes, but with `client.chat(stream=True)`\n",
    "- **Thinking visibility** - Yes, but via `<think>` tag parsing\n",
    "- **Progressive display** - Yes, but with chunk iteration\n",
    "- **Real-time feedback** - Yes, but simpler implementation\n",
    "\n",
    "### üéØ **Key Difference:**\n",
    "**Anthropic**: Complex event-driven streaming with budget management  \n",
    "**DeepSeek-R1**: Simple chunk-based streaming with automatic thinking\n",
    "\n",
    "**The concepts are similar, but the implementation is completely different.** DeepSeek-R1 is actually simpler - no complex event handling needed, just parse the `<think>` tags from the streaming content!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eff125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANTHROPIC STREAMING (DOESN'T WORK WITH DEEPSEEK)\n",
    "# ============================================================================\n",
    "\n",
    "def anthropic_style_streaming():\n",
    "    \"\"\"This is how Anthropic streaming works - NOT valid for DeepSeek\"\"\"\n",
    "    \n",
    "    # ‚ùå This structure doesn't exist in Ollama\n",
    "    \"\"\"\n",
    "    with client.messages.stream(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        thinking={\"type\": \"enabled\", \"budget_tokens\": 5000},\n",
    "        messages=[{\"role\": \"user\", \"content\": \"...\"}]\n",
    "    ) as stream:\n",
    "        for event in stream:\n",
    "            if event.type == \"content_block_start\":\n",
    "                # Anthropic-specific events\n",
    "            elif event.type == \"content_block_delta\":\n",
    "                # Anthropic-specific deltas\n",
    "    \"\"\"\n",
    "    print(\"‚ùå This Anthropic pattern doesn't work with DeepSeek-R1/Ollama\")\n",
    "\n",
    "# ============================================================================\n",
    "# DEEPSEEK-R1 STREAMING (WHAT ACTUALLY WORKS)\n",
    "# ============================================================================\n",
    "\n",
    "def deepseek_streaming_correct():\n",
    "    \"\"\"This is how DeepSeek-R1 streaming actually works\"\"\"\n",
    "    \n",
    "    print(\"‚úÖ DeepSeek-R1 Streaming (Correct Method)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # ‚úÖ This is the correct way for DeepSeek-R1\n",
    "    stream = client.chat(\n",
    "        model='deepseek-r1:14b',\n",
    "        messages=[{\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Explain quantum computing in simple terms.\"\n",
    "        }],\n",
    "        stream=True  # Simple boolean flag\n",
    "    )\n",
    "    \n",
    "    # Track streaming state\n",
    "    full_response = \"\"\n",
    "    in_thinking = False\n",
    "    \n",
    "    print(\"ü§ñ DeepSeek-R1: \", end=\"\", flush=True)\n",
    "    \n",
    "    # ‚úÖ Simple iteration over chunks\n",
    "    for chunk in stream:\n",
    "        if 'message' in chunk:\n",
    "            content = chunk['message'].get('content', '')\n",
    "            full_response += content\n",
    "            \n",
    "            # Handle thinking transitions\n",
    "            if '<think>' in content and not in_thinking:\n",
    "                in_thinking = True\n",
    "                print(\"\\nüß† [Thinking...] \", end=\"\", flush=True)\n",
    "                content = content.replace('<think>', '')\n",
    "            \n",
    "            if '</think>' in content and in_thinking:\n",
    "                in_thinking = False\n",
    "                print(\" [Done]\\nüí° Answer: \", end=\"\", flush=True)\n",
    "                content = content.replace('</think>', '')\n",
    "            \n",
    "            # Show appropriate content\n",
    "            if in_thinking:\n",
    "                print(\".\", end=\"\", flush=True)  # Progress dots\n",
    "            else:\n",
    "                print(content, end=\"\", flush=True)  # Actual content\n",
    "    \n",
    "    print(f\"\\n\\n‚úÖ Complete! ({len(full_response)} chars)\")\n",
    "    return full_response\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARISON TABLE\n",
    "# ============================================================================\n",
    "\n",
    "def show_streaming_comparison():\n",
    "    \"\"\"Show the differences between Anthropic and DeepSeek streaming\"\"\"\n",
    "    \n",
    "    print(\"\\nüìä STREAMING API COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    comparison = [\n",
    "        {\n",
    "            \"Feature\": \"Stream initiation\",\n",
    "            \"Anthropic\": \"client.messages.stream()\",\n",
    "            \"DeepSeek\": \"client.chat(stream=True)\"\n",
    "        },\n",
    "        {\n",
    "            \"Feature\": \"Context manager\", \n",
    "            \"Anthropic\": \"with stream as s:\",\n",
    "            \"DeepSeek\": \"for chunk in stream:\"\n",
    "        },\n",
    "        {\n",
    "            \"Feature\": \"Event types\",\n",
    "            \"Anthropic\": \"event.type, event.delta\",\n",
    "            \"DeepSeek\": \"chunk['message']['content']\"\n",
    "        },\n",
    "        {\n",
    "            \"Feature\": \"Thinking detection\",\n",
    "            \"Anthropic\": \"event.type == 'thinking'\",\n",
    "            \"DeepSeek\": \"Parse <think> tags\"\n",
    "        },\n",
    "        {\n",
    "            \"Feature\": \"Content access\",\n",
    "            \"Anthropic\": \"event.delta.text\",\n",
    "            \"DeepSeek\": \"chunk['message']['content']\"\n",
    "        },\n",
    "        {\n",
    "            \"Feature\": \"Thinking budget\",\n",
    "            \"Anthropic\": \"budget_tokens=5000\",\n",
    "            \"DeepSeek\": \"Automatic (no config needed)\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(f\"{'Feature':<20} | {'Anthropic':<25} | {'DeepSeek-R1':<25}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for comp in comparison:\n",
    "        print(f\"{comp['Feature']:<20} | {comp['Anthropic']:<25} | {comp['DeepSeek']:<25}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PRACTICAL DEEPSEEK STREAMING EXAMPLES\n",
    "# ============================================================================\n",
    "\n",
    "def practical_streaming_examples():\n",
    "    \"\"\"Show practical streaming patterns for DeepSeek-R1\"\"\"\n",
    "    \n",
    "    print(f\"\\nüöÄ PRACTICAL DEEPSEEK STREAMING PATTERNS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Pattern 1: Simple streaming\n",
    "    print(f\"\\n1Ô∏è‚É£ Simple Streaming:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(\"\"\"\n",
    "stream = client.chat(model='deepseek-r1:14b', messages=messages, stream=True)\n",
    "for chunk in stream:\n",
    "    if 'message' in chunk:\n",
    "        content = chunk['message'].get('content', '')\n",
    "        print(content, end='', flush=True)\n",
    "\"\"\")\n",
    "    \n",
    "    # Pattern 2: Thinking-aware streaming\n",
    "    print(f\"\\n2Ô∏è‚É£ Thinking-Aware Streaming:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(\"\"\"\n",
    "in_thinking = False\n",
    "for chunk in stream:\n",
    "    content = chunk['message'].get('content', '')\n",
    "    if '<think>' in content: in_thinking = True\n",
    "    if '</think>' in content: in_thinking = False\n",
    "    \n",
    "    if in_thinking:\n",
    "        print('.', end='', flush=True)  # Progress\n",
    "    else:\n",
    "        print(content, end='', flush=True)  # Response\n",
    "\"\"\")\n",
    "    \n",
    "    # Pattern 3: Advanced streaming with analysis\n",
    "    print(f\"\\n3Ô∏è‚É£ Advanced Streaming with Analysis:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(\"\"\"\n",
    "thinking_buffer = \"\"\n",
    "response_buffer = \"\"\n",
    "\n",
    "for chunk in stream:\n",
    "    content = chunk['message'].get('content', '')\n",
    "    \n",
    "    if in_thinking_mode:\n",
    "        thinking_buffer += content\n",
    "        show_thinking_progress()\n",
    "    else:\n",
    "        response_buffer += content\n",
    "        display_response(content)\n",
    "\n",
    "analyze_thinking_patterns(thinking_buffer)\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# RUN EXAMPLES\n",
    "# ============================================================================\n",
    "\n",
    "def run_all_examples():\n",
    "    \"\"\"Run all streaming examples\"\"\"\n",
    "    \n",
    "    print(\"üéØ DEEPSEEK-R1 STREAMING GUIDE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Show what doesn't work\n",
    "    anthropic_style_streaming()\n",
    "    \n",
    "    # Show what does work\n",
    "    result = deepseek_streaming_correct()\n",
    "    \n",
    "    # Show comparisons\n",
    "    show_streaming_comparison()\n",
    "    \n",
    "    # Show practical patterns\n",
    "    practical_streaming_examples()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Key Takeaway: DeepSeek-R1 streaming is simpler!\")\n",
    "    print(f\"   No complex event handling - just parse <think> tags\")\n",
    "    print(f\"   No budget management - thinking is automatic\")\n",
    "    print(f\"   No special context managers - simple iteration\")\n",
    "\n",
    "# Run the complete guide\n",
    "run_all_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4e60c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_thinking_example():\n",
    "    \"\"\"Demonstrate streaming with DeepSeek-R1 thinking\"\"\"\n",
    "    \n",
    "    print(\"üåä Streaming DeepSeek-R1 Thinking Example\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    question = \"\"\"Design a simple REST API for a todo list application. \n",
    "    Include endpoints for CRUD operations and consider:\n",
    "    - Authentication\n",
    "    - Error handling\n",
    "    - Data validation\n",
    "    - Response formats\"\"\"\n",
    "    \n",
    "    print(f\"‚ùì Question: {question}\\n\")\n",
    "    \n",
    "    # Stream the response\n",
    "    stream = client.chat(\n",
    "        model='deepseek-r1:14b',\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        }],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    # Track the response as it comes in\n",
    "    full_response = \"\"\n",
    "    in_thinking = False\n",
    "    thinking_content = \"\"\n",
    "    final_answer = \"\"\n",
    "    \n",
    "    print(\"ü§ñ DeepSeek-R1 Response:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for chunk in stream:\n",
    "        if 'message' in chunk:\n",
    "            content = chunk['message'].get('content', '')\n",
    "            full_response += content\n",
    "            \n",
    "            # Check if we're entering thinking mode\n",
    "            if '<think>' in content and not in_thinking:\n",
    "                in_thinking = True\n",
    "                print(\"\\nüß† [THINKING...] \", end=\"\", flush=True)\n",
    "                continue\n",
    "            \n",
    "            # Check if we're exiting thinking mode\n",
    "            if '</think>' in content and in_thinking:\n",
    "                in_thinking = False\n",
    "                print(\" [THINKING COMPLETE]\\n\")\n",
    "                print(\"üí° Final Answer:\")\n",
    "                print(\"-\" * 20)\n",
    "                continue\n",
    "            \n",
    "            # Show progress dots during thinking\n",
    "            if in_thinking:\n",
    "                print(\".\", end=\"\", flush=True)\n",
    "            else:\n",
    "                # Show the actual final answer\n",
    "                print(content, end=\"\", flush=True)\n",
    "    \n",
    "    print(f\"\\n\\nüìä Stream Summary:\")\n",
    "    print(f\"  Total response length: {len(full_response)} characters\")\n",
    "    \n",
    "    # Parse the complete response for detailed analysis\n",
    "    if '<think>' in full_response and '</think>' in full_response:\n",
    "        thinking_start = full_response.find('<think>') + 7\n",
    "        thinking_end = full_response.find('</think>')\n",
    "        thinking_content = full_response[thinking_start:thinking_end].strip()\n",
    "        final_answer = full_response[thinking_end + 8:].strip()\n",
    "        \n",
    "        print(f\"  Thinking length: {len(thinking_content)} characters\")\n",
    "        print(f\"  Final answer length: {len(final_answer)} characters\")\n",
    "        print(f\"  Thinking took: {len(thinking_content.split())} words to process\")\n",
    "    \n",
    "    return full_response\n",
    "\n",
    "# Run the streaming example\n",
    "result = stream_thinking_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7c5c5f",
   "metadata": {},
   "source": [
    "### 5.2 Extended Thinking with Tool Use\n",
    "\n",
    "Extended thinking can be combined with tool use for even more powerful applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc5d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "def advanced_streaming_with_thinking():\n",
    "    \"\"\"Advanced streaming that shows thinking process in real-time\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Advanced Streaming with Real-time Thinking Display\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    question = \"\"\"Analyze this scenario: A tech startup wants to implement AI in their \n",
    "    customer service. They have 10,000 daily support tickets, 60% are simple FAQ-type \n",
    "    questions, 30% require human judgment, and 10% are complex technical issues. \n",
    "    Design a solution considering costs, customer satisfaction, and implementation timeline.\"\"\"\n",
    "    \n",
    "    print(f\"‚ùì Complex Question: {question}\\n\")\n",
    "    \n",
    "    # Stream the response\n",
    "    stream = client.chat(\n",
    "        model='deepseek-r1:14b',\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        }],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    # Advanced tracking\n",
    "    full_response = \"\"\n",
    "    thinking_buffer = \"\"\n",
    "    final_buffer = \"\"\n",
    "    in_thinking = False\n",
    "    thinking_started = False\n",
    "    \n",
    "    print(\"ü§ñ DeepSeek-R1 Processing:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for chunk in stream:\n",
    "        if 'message' in chunk:\n",
    "            content = chunk['message'].get('content', '')\n",
    "            full_response += content\n",
    "            \n",
    "            # Handle thinking block start\n",
    "            if '<think>' in content:\n",
    "                in_thinking = True\n",
    "                thinking_started = True\n",
    "                content = content.replace('<think>', '')\n",
    "                print(\"\\nüß† THINKING PROCESS:\")\n",
    "                print(\"-\" * 30)\n",
    "                time.sleep(0.1)  # Small delay for readability\n",
    "            \n",
    "            # Handle thinking block end\n",
    "            if '</think>' in content:\n",
    "                in_thinking = False\n",
    "                content = content.replace('</think>', '')\n",
    "                thinking_buffer += content\n",
    "                print(f\"\\n{'.'*30}\")\n",
    "                print(\"üí° FINAL SOLUTION:\")\n",
    "                print(\"-\" * 30)\n",
    "                time.sleep(0.2)\n",
    "                continue\n",
    "            \n",
    "            # Process content based on current state\n",
    "            if in_thinking:\n",
    "                thinking_buffer += content\n",
    "                # Show thinking in real-time with slight delay\n",
    "                for char in content:\n",
    "                    print(char, end='', flush=True)\n",
    "                    time.sleep(0.005)  # Very small delay for dramatic effect\n",
    "            else:\n",
    "                final_buffer += content\n",
    "                # Show final answer immediately\n",
    "                print(content, end='', flush=True)\n",
    "    \n",
    "    # Final analysis\n",
    "    print(f\"\\n\\nüìä ADVANCED ANALYSIS:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    if thinking_buffer:\n",
    "        print(f\"‚úÖ Thinking captured: {len(thinking_buffer)} characters\")\n",
    "        print(f\"‚úÖ Final answer: {len(final_buffer)} characters\")\n",
    "        print(f\"‚úÖ Thinking-to-answer ratio: {len(thinking_buffer)/len(final_buffer):.1f}:1\")\n",
    "        \n",
    "        # Analyze thinking patterns\n",
    "        thinking_sentences = thinking_buffer.split('.')\n",
    "        print(f\"‚úÖ Thinking sentences: {len(thinking_sentences)}\")\n",
    "        \n",
    "        # Look for solution patterns\n",
    "        solution_keywords = ['consider', 'implement', 'solution', 'approach', 'strategy', 'recommend']\n",
    "        found_patterns = [kw for kw in solution_keywords if kw.lower() in thinking_buffer.lower()]\n",
    "        print(f\"‚úÖ Solution patterns found: {found_patterns}\")\n",
    "        \n",
    "        # Show thinking summary\n",
    "        print(f\"\\nüéØ THINKING SUMMARY (first 300 chars):\")\n",
    "        print(f\"   {thinking_buffer[:300]}...\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No thinking block detected\")\n",
    "    \n",
    "    return {\n",
    "        'full_response': full_response,\n",
    "        'thinking': thinking_buffer,\n",
    "        'final_answer': final_buffer,\n",
    "        'has_thinking': bool(thinking_buffer)\n",
    "    }\n",
    "\n",
    "# Run the advanced streaming example\n",
    "result = advanced_streaming_with_thinking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e941b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_thinking_example():\n",
    "    \"\"\"Demonstrate streaming with DeepSeek-R1 extended thinking\"\"\"\n",
    "    \n",
    "    print(\"üåä Streaming Extended Thinking Example\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # DeepSeek-R1 automatically provides thinking - no budget needed!\n",
    "    stream = client.chat(\n",
    "        model='deepseek-r1:14b',\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"Design a simple REST API for a todo list application. \n",
    "            Include endpoints for CRUD operations and consider:\n",
    "            - Authentication\n",
    "            - Error handling\n",
    "            - Data validation\n",
    "            - Response formats\"\"\"\n",
    "        }],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    # Track streaming state\n",
    "    in_thinking = False\n",
    "    full_response = \"\"\n",
    "    \n",
    "    for chunk in stream:\n",
    "        if 'message' in chunk:\n",
    "            content = chunk['message'].get('content', '')\n",
    "            full_response += content\n",
    "            \n",
    "            # Detect thinking block start\n",
    "            if '<think>' in content and not in_thinking:\n",
    "                in_thinking = True\n",
    "                print(\"\\nü§î DeepSeek is thinking...\", end=\"\", flush=True)\n",
    "                content = content.replace('<think>', '')\n",
    "            \n",
    "            # Detect thinking block end\n",
    "            if '</think>' in content and in_thinking:\n",
    "                in_thinking = False\n",
    "                content = content.replace('</think>', '')\n",
    "                print(\" Done thinking!\")\n",
    "                print(\"\\n\\n‚úÖ Final Response:\\n\", end=\"\", flush=True)\n",
    "            \n",
    "            # Show appropriate output\n",
    "            if in_thinking:\n",
    "                # Show progress dots during thinking\n",
    "                print(\".\", end=\"\", flush=True)\n",
    "            else:\n",
    "                # Show the actual response content\n",
    "                print(content, end=\"\", flush=True)\n",
    "    \n",
    "    print(f\"\\n\\nResponse complete! Total length: {len(full_response)} characters\")\n",
    "    return full_response\n",
    "\n",
    "# Run the example\n",
    "result = stream_thinking_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8706d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thinking_with_tools_example():\n",
    "    \"\"\"Demonstrate extended thinking with simulated tool use\"\"\"\n",
    "    \n",
    "    # Note: DeepSeek-R1 doesn't have native tool calling like Claude,\n",
    "    # but we can simulate it by asking it to \"think through\" calculations\n",
    "    \n",
    "    response = client.chat(\n",
    "        model='deepseek-r1:14b',\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"I'm planning a party for 25 people. Each person will eat:\n",
    "            - 3 slices of pizza (8 slices per pizza)\n",
    "            - 2 sodas ($1.50 each)\n",
    "            - 1 dessert ($3.00 each)\n",
    "            \n",
    "            Pizzas cost $12 each. Calculate the total cost and quantities needed.\n",
    "            Please show your mathematical calculations step by step.\"\"\"\n",
    "        }],\n",
    "        stream=False\n",
    "    )\n",
    "    \n",
    "    print(\"üéâ Party Planning with Extended Thinking\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    full_response = response.get('message', {}).get('content', '')\n",
    "    \n",
    "    # Parse thinking and final answer\n",
    "    if '<think>' in full_response and '</think>' in full_response:\n",
    "        thinking_start = full_response.find('<think>') + 7\n",
    "        thinking_end = full_response.find('</think>')\n",
    "        thinking_content = full_response[thinking_start:thinking_end].strip()\n",
    "        final_answer = full_response[thinking_end + 8:].strip()\n",
    "        \n",
    "        print(\"\\nü§î Planning Process:\")\n",
    "        print(\"-\" * 40)\n",
    "        # Show first 1000 characters of thinking\n",
    "        print(thinking_content[:1000])\n",
    "        if len(thinking_content) > 1000:\n",
    "            print(\"...\\n\")\n",
    "        \n",
    "        print(\"\\nüìã Final Plan:\")\n",
    "        print(\"-\" * 40)\n",
    "        display(Markdown(final_answer))\n",
    "        \n",
    "        # Analyze the thinking for mathematical patterns\n",
    "        calc_keywords = ['calculate', 'multiply', 'add', 'total', 'cost', 'pizza', 'soda']\n",
    "        found_calcs = [kw for kw in calc_keywords if kw.lower() in thinking_content.lower()]\n",
    "        print(f\"\\nüîß Mathematical concepts used: {found_calcs}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nüìã Response:\")\n",
    "        display(Markdown(full_response))\n",
    "    \n",
    "    return full_response\n",
    "\n",
    "# DeepSeek-R1 Token Guidelines (No explicit budget needed)\n",
    "print(\"üéØ DeepSeek-R1 Guidelines\")\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖ No token budgets needed - thinking is automatic\")\n",
    "print(\"‚úÖ Thinking depth scales with problem complexity\")\n",
    "print(\"‚úÖ Average thinking: 2K-10K characters\")\n",
    "print(\"‚úÖ Complex problems: 10K-50K+ characters\")\n",
    "print(\"‚úÖ No additional cost for thinking tokens\")\n",
    "\n",
    "thinking_with_tools_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4333e4f",
   "metadata": {},
   "source": [
    "<a id='best-practices'></a>\n",
    "## 6. Best Practices\n",
    "\n",
    "### 6.1 Choosing the Right Budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e0fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompting_best_practices():\n",
    "    \"\"\"Demonstrate effective prompting strategies for DeepSeek-R1\"\"\"\n",
    "    \n",
    "    # Good prompt - clear, specific, structured\n",
    "    good_prompt = \"\"\"Analyze the following investment options and recommend the best choice:\n",
    "\n",
    "Option A: Stock Portfolio\n",
    "- Expected annual return: 8%\n",
    "- Risk level: High\n",
    "- Minimum investment: $10,000\n",
    "- Liquidity: High (can sell anytime)\n",
    "\n",
    "Option B: Real Estate\n",
    "- Expected annual return: 6%\n",
    "- Risk level: Medium\n",
    "- Minimum investment: $50,000\n",
    "- Liquidity: Low (takes months to sell)\n",
    "\n",
    "Option C: Bonds\n",
    "- Expected annual return: 4%\n",
    "- Risk level: Low\n",
    "- Minimum investment: $5,000\n",
    "- Liquidity: Medium\n",
    "\n",
    "Investor Profile:\n",
    "- Age: 35\n",
    "- Investment horizon: 15 years\n",
    "- Risk tolerance: Medium\n",
    "- Available capital: $75,000\n",
    "- Goal: Retirement savings\n",
    "\n",
    "Please provide:\n",
    "1. Analysis of each option\n",
    "2. Recommended allocation\n",
    "3. Justification for your recommendation\n",
    "\n",
    "Think through this systematically, considering risk-return tradeoffs, \n",
    "diversification principles, and the investor's specific situation.\"\"\"\n",
    "    \n",
    "    response = client.chat(\n",
    "        model='deepseek-r1:14b',\n",
    "        messages=[{\"role\": \"user\", \"content\": good_prompt}],\n",
    "        stream=False\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Best Practices Example: Structured Investment Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    full_response = response.get('message', {}).get('content', '')\n",
    "    \n",
    "    # Parse and display thinking + answer\n",
    "    if '<think>' in full_response and '</think>' in full_response:\n",
    "        thinking_start = full_response.find('<think>') + 7\n",
    "        thinking_end = full_response.find('</think>')\n",
    "        thinking_content = full_response[thinking_start:thinking_end].strip()\n",
    "        final_answer = full_response[thinking_end + 8:].strip()\n",
    "        \n",
    "        print(\"\\nüß† THINKING PROCESS:\")\n",
    "        print(\"=\" * 40)\n",
    "        # Show key parts of thinking\n",
    "        print(thinking_content[:800] + \"...\" if len(thinking_content) > 800 else thinking_content)\n",
    "        \n",
    "        print(f\"\\nüí° FINAL RECOMMENDATION:\")\n",
    "        print(\"=\" * 40)\n",
    "        display(Markdown(final_answer))\n",
    "        \n",
    "        # Analyze thinking quality\n",
    "        analysis_keywords = ['risk', 'return', 'diversification', 'allocation', 'horizon', 'liquidity']\n",
    "        found_concepts = [kw for kw in analysis_keywords if kw.lower() in thinking_content.lower()]\n",
    "        print(f\"\\nüìä Investment concepts analyzed: {found_concepts}\")\n",
    "        \n",
    "    else:\n",
    "        display(Markdown(full_response))\n",
    "    \n",
    "    return full_response\n",
    "\n",
    "def show_prompting_tips():\n",
    "    \"\"\"Show DeepSeek-R1 specific prompting tips\"\"\"\n",
    "    \n",
    "    print(\"\\nüéØ DeepSeek-R1 Prompting Tips\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    tips = [\n",
    "        {\n",
    "            \"tip\": \"Be Explicit About Thinking\",\n",
    "            \"description\": \"Ask to 'think through systematically' or 'analyze step by step'\",\n",
    "            \"example\": \"'Think through this problem step by step before giving your answer'\"\n",
    "        },\n",
    "        {\n",
    "            \"tip\": \"Structure Your Requests\", \n",
    "            \"description\": \"Use numbered lists and clear sections\",\n",
    "            \"example\": \"'Please provide: 1. Analysis 2. Recommendation 3. Justification'\"\n",
    "        },\n",
    "        {\n",
    "            \"tip\": \"Provide Context\",\n",
    "            \"description\": \"Give background information and constraints\",\n",
    "            \"example\": \"'Consider the investor profile and 15-year time horizon'\"\n",
    "        },\n",
    "        {\n",
    "            \"tip\": \"Ask for Reasoning\",\n",
    "            \"description\": \"Request explanations of the thought process\",\n",
    "            \"example\": \"'Explain your reasoning and show your calculations'\"\n",
    "        },\n",
    "        {\n",
    "            \"tip\": \"Specify Output Format\",\n",
    "            \"description\": \"Request specific formats or structures\",\n",
    "            \"example\": \"'Provide a summary table and detailed analysis'\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, tip in enumerate(tips, 1):\n",
    "        print(f\"\\n{i}. {tip['tip']}\")\n",
    "        print(f\"   üìù {tip['description']}\")\n",
    "        print(f\"   üí° Example: {tip['example']}\")\n",
    "    \n",
    "    print(f\"\\n‚ú® Pro Tip: DeepSeek-R1 excels at mathematical reasoning,\")\n",
    "    print(f\"   financial analysis, and systematic problem-solving!\")\n",
    "\n",
    "# Run the examples\n",
    "result = prompting_best_practices()\n",
    "show_prompting_tips()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e39a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepseek_cost_benefits():\n",
    "    \"\"\"Analyze DeepSeek-R1 cost benefits vs Claude thinking\"\"\"\n",
    "    \n",
    "    print(\"üí∞ DeepSeek-R1 vs Claude Thinking Cost Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Simulated costs (DeepSeek-R1 via Ollama is FREE!)\n",
    "    scenarios = [\n",
    "        {\"name\": \"Simple Analysis\", \"input\": 500, \"thinking\": 5000, \"output\": 1000},\n",
    "        {\"name\": \"Complex Problem\", \"input\": 2000, \"thinking\": 20000, \"output\": 3000},\n",
    "        {\"name\": \"Deep Research\", \"input\": 5000, \"thinking\": 50000, \"output\": 8000}\n",
    "    ]\n",
    "    \n",
    "    # Claude pricing (example - per million tokens)\n",
    "    claude_pricing = {\"input\": 3, \"output\": 15}\n",
    "    \n",
    "    print(f\"\\nüìä Cost Comparison:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    total_claude_cost = 0\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        # Claude costs (thinking billed as output)\n",
    "        claude_input_cost = (scenario[\"input\"] / 1_000_000) * claude_pricing[\"input\"]\n",
    "        claude_thinking_cost = (scenario[\"thinking\"] / 1_000_000) * claude_pricing[\"output\"]\n",
    "        claude_output_cost = (scenario[\"output\"] / 1_000_000) * claude_pricing[\"output\"]\n",
    "        claude_total = claude_input_cost + claude_thinking_cost + claude_output_cost\n",
    "        total_claude_cost += claude_total\n",
    "        \n",
    "        # DeepSeek-R1 costs (FREE with local Ollama!)\n",
    "        deepseek_cost = 0.00\n",
    "        \n",
    "        print(f\"\\n  {scenario['name']}:\")\n",
    "        print(f\"    Input: {scenario['input']:,} | Thinking: {scenario['thinking']:,} | Output: {scenario['output']:,}\")\n",
    "        print(f\"    Claude cost: ${claude_total:.4f}\")\n",
    "        print(f\"    DeepSeek-R1: ${deepseek_cost:.2f} (FREE!)\")\n",
    "        print(f\"    Savings: ${claude_total:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüí° TOTAL SAVINGS: ${total_claude_cost:.4f} per analysis cycle\")\n",
    "    print(f\"üéØ Monthly savings (100 analyses): ${total_claude_cost * 100:.2f}\")\n",
    "    print(f\"üöÄ Annual savings (1200 analyses): ${total_claude_cost * 1200:.2f}\")\n",
    "\n",
    "def deepseek_advantages():\n",
    "    \"\"\"Show DeepSeek-R1 advantages\"\"\"\n",
    "    \n",
    "    print(f\"\\n‚≠ê DeepSeek-R1 Advantages\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    advantages = [\n",
    "        {\n",
    "            \"category\": \"üí∞ Cost\",\n",
    "            \"points\": [\n",
    "                \"Completely FREE when run locally\",\n",
    "                \"No token counting or budget management\",\n",
    "                \"No API costs or rate limits\",\n",
    "                \"One-time setup, unlimited usage\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"üß† Thinking Quality\", \n",
    "            \"points\": [\n",
    "                \"Deep reasoning automatically included\",\n",
    "                \"Visible thought process in <think> tags\",\n",
    "                \"Excellent at mathematical reasoning\",\n",
    "                \"Strong logical problem-solving\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"üîí Privacy\",\n",
    "            \"points\": [\n",
    "                \"Runs entirely on your hardware\",\n",
    "                \"No data sent to external APIs\",\n",
    "                \"Complete control over your data\",\n",
    "                \"Enterprise-safe deployment\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"‚ö° Performance\",\n",
    "            \"points\": [\n",
    "                \"No network latency (local inference)\",\n",
    "                \"Consistent availability\",\n",
    "                \"Customizable parameters\",\n",
    "                \"GPU acceleration support\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for advantage in advantages:\n",
    "        print(f\"\\n{advantage['category']}\")\n",
    "        for point in advantage['points']:\n",
    "            print(f\"  ‚úÖ {point}\")\n",
    "\n",
    "def performance_comparison():\n",
    "    \"\"\"Compare performance characteristics\"\"\"\n",
    "    \n",
    "    print(f\"\\nüìà Performance Characteristics\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    comparison = [\n",
    "        {\"metric\": \"Cost per analysis\", \"claude\": \"$0.05-0.50\", \"deepseek\": \"$0.00\"},\n",
    "        {\"metric\": \"Thinking transparency\", \"claude\": \"Summary only\", \"deepseek\": \"Full process\"},\n",
    "        {\"metric\": \"Setup complexity\", \"claude\": \"API key only\", \"deepseek\": \"Local install\"},\n",
    "        {\"metric\": \"Data privacy\", \"claude\": \"Cloud-based\", \"deepseek\": \"Local only\"},\n",
    "        {\"metric\": \"Token limits\", \"claude\": \"Budget required\", \"deepseek\": \"No limits\"},\n",
    "        {\"metric\": \"Availability\", \"claude\": \"Internet required\", \"deepseek\": \"Always local\"}\n",
    "    ]\n",
    "    \n",
    "    print(f\"{'Metric':<20} | {'Claude':<15} | {'DeepSeek-R1':<15}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for comp in comparison:\n",
    "        print(f\"{comp['metric']:<20} | {comp['claude']:<15} | {comp['deepseek']:<15}\")\n",
    "\n",
    "# Run all analyses\n",
    "deepseek_cost_benefits()\n",
    "deepseek_advantages() \n",
    "performance_comparison()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-reasoning-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
