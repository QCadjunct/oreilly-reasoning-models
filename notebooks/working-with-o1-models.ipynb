{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are seven concise, concrete examples—one for each recommended use case from the tutorial. Each example has:\n",
    "\n",
    "1. **Problem Description** (the scenario)\n",
    "2. **Prompt** (as would be sent to the reasoning model—using direct, concise instructions)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Navigating ambiguous tasks\n",
    "\n",
    "**Problem Description**  \n",
    "A startup founder needs to draft hiring guidelines for a new position but provides only partial and somewhat conflicting requirements. They’re unsure what skill level they can afford versus what they need. The model must interpret the user’s incomplete constraints, clarify them, and propose a solution.\n",
    "\n",
    "**Prompt**  \n",
    "```\n",
    "[Developer Message]\n",
    "Formatting re-enabled\n",
    "You are a reasoning model. Our user wants to create a hiring plan for a UX designer but only has partial guidelines:\n",
    "\n",
    "- Their budget is uncertain\n",
    "- They want a senior-level skill set\n",
    "- They have conflicting timelines (start immediately vs. wait 3 months)\n",
    "- They’re unsure if they need a permanent or contract hire\n",
    "\n",
    "Please reconcile these requirements, highlight areas needing clarification, and propose a short, workable hiring plan.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m client = \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m response = client.chat.completions.create(\n\u001b[32m      6\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mo1-preview\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     messages=[\n\u001b[32m      8\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mHello, world!\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m      9\u001b[39m     ]\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mz:\\VSCODE Projects\\PythonProjects\\oreilly-reasoning-models\\oreilly-reasoning-models\\.venv\\Lib\\site-packages\\openai\\_client.py:130\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    128\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    131\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m     )\n\u001b[32m    133\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o1-preview\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello, world!\"}\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "[Developer Message]\n",
    "Formatting re-enabled\n",
    "You are a reasoning model. Our user wants to create a hiring plan for a UX designer but only has partial guidelines:\n",
    "\n",
    "- Their budget is uncertain\n",
    "- They want a senior-level skill set\n",
    "- They have conflicting timelines (start immediately vs. wait 3 months)\n",
    "- They’re unsure if they need a permanent or contract hire\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o1-preview\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Finding a needle in a haystack\n",
    "\n",
    "**Problem Description**  \n",
    "A legal team has a 50-page contract with many footnotes. Buried in the footnotes is a “change of control” clause that triggers immediate debt repayment. The model must search all text and highlight that hidden clause.\n",
    "\n",
    "**Prompt**  \n",
    "```\n",
    "[Developer Message]\n",
    "Formatting re-enabled\n",
    "Below is a 50-page contract excerpt. Identify any clauses that reference “change of control,” summarize the key conditions from them, and highlight financial obligations that might be triggered.\n",
    "\n",
    "---\n",
    "CONTRACT TEXT:\n",
    "(Full 50-page text goes here; possibly including footnotes)\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Finding relationships and nuance across a large dataset\n",
    "\n",
    "**Problem Description**  \n",
    "A tax expert wants to reconcile multiple financial documents (tax code references, expense logs, corporate structure charts) to determine how a new local tax rule applies to overseas transactions. The relevant rule isn’t explicitly mentioned in any single document but can be inferred from cross-references.\n",
    "\n",
    "**Prompt**  \n",
    "```\n",
    "[Developer Message]\n",
    "Formatting re-enabled\n",
    "You have these documents:\n",
    "1) Excerpts from local tax codes\n",
    "2) Corporate expense logs\n",
    "3) An organizational chart spanning multiple countries\n",
    "\n",
    "Determine how a newly introduced local tax rule applies to the overseas branches of the company. Show your final conclusion in concise bullet points. Cross-reference relevant segments of each document if needed.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Multi-step agentic planning\n",
    "\n",
    "**Problem Description**  \n",
    "A user wants to design a multi-step marketing campaign: identifying the target audience, planning the budget, creating ad copy, and choosing the ad platform. They also want to automate each step if possible.\n",
    "\n",
    "**Prompt**  \n",
    "```\n",
    "[Developer Message]\n",
    "Formatting re-enabled\n",
    "Act as the planner for this multi-step marketing campaign. Break the problem into steps:\n",
    "1) Identify the audience segments\n",
    "2) Draft a realistic budget\n",
    "3) Propose initial ad copy\n",
    "4) Select the best ad platforms\n",
    "\n",
    "For each step, assign it either to a reasoning model (if complex) or a GPT model (if faster/cheaper). Produce a task list with justifications.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Visual reasoning\n",
    "\n",
    "**Problem Description**  \n",
    "An architect has multiple blueprint pages. Page 1 includes a legend of architectural symbols. Page 2 shows the floor plan, referencing the symbols. The model must interpret the legend and figure out the materials indicated in the drawing—like identifying a “4x4 PT post” as a pressure-treated wood post.\n",
    "\n",
    "**Prompt**  \n",
    "```\n",
    "[Developer Message]\n",
    "Formatting re-enabled\n",
    "We have two images representing architectural blueprints:\n",
    "\n",
    "- Image A: Legend of symbols\n",
    "- Image B: Floor plan with the same symbols\n",
    "\n",
    "Identify the materials and structural components on the floor plan, referencing the legend. Output a list of each symbol and the corresponding material. If the legend uses abbreviations (like “PT” for pressure treated), apply it correctly in the final list.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Reviewing, debugging, and improving code quality\n",
    "\n",
    "**Problem Description**  \n",
    "A developer has submitted a pull request with new features across several files in a large codebase. The model must perform an in-depth code review to identify potential bugs, performance bottlenecks, and coding standard violations.\n",
    "\n",
    "**Prompt**  \n",
    "```\n",
    "[Developer Message]\n",
    "Formatting re-enabled\n",
    "Here are the diffs from multiple files in a large codebase (pseudocode or actual diffs). Perform a thorough code review. For each file:\n",
    "1) Flag any possible logic errors or performance issues\n",
    "2) Suggest improvements or best practices\n",
    "3) Indicate if you see any style inconsistencies\n",
    "\n",
    "Provide your feedback as a bullet list organized by filename.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Evaluation and benchmarking for other model responses\n",
    "\n",
    "**Problem Description**  \n",
    "A user has two different model-generated summaries of patient questions. They want to evaluate which summary is more accurate, consistent, and comprehensive, according to strict healthcare guidelines.\n",
    "\n",
    "**Prompt**  \n",
    "```\n",
    "[Developer Message]\n",
    "Formatting re-enabled\n",
    "We have two candidate summaries generated by separate models. Evaluate each summary on:\n",
    "- Accuracy of medical facts\n",
    "- Coverage of all patient concerns\n",
    "- Consistency with our policy guidelines\n",
    "\n",
    "Provide a scoring or ranking for each category. Then recommend which summary is better overall.  \n",
    "Candidate Summaries:\n",
    "A) ...\n",
    "B) ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "These examples demonstrate how to use reasoning models (like `o1` or `o3-mini`) in different scenarios, emphasizing clarity, concise prompts, and a direct request for outcomes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-reasoning-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
